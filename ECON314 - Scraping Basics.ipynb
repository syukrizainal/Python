{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "de7f445b",
   "metadata": {},
   "source": [
    "ECON 314 – Assignment 2 – Scraping Basics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "741be06b",
   "metadata": {},
   "source": [
    "1.\tUse python to extract ANZ's Currency rates from https://www.interest.co.nz/currencies/buying-foreign-currency \n",
    "    a.\tThe result should be a list with just the numerical rates of ANZ for the various currencies!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e2e10354-d6d1-47d5-a77c-97026c92cf36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.9391, 0.8679, 0.6272, 0.5841, 1.3959, 0.4978, 5.3842, 50.9463, 0.9349, 22.8557, 0.6929, 75.89, 9.9325]\n"
     ]
    }
   ],
   "source": [
    "# getting the right programs that will allow us to scrape\n",
    "from urllib.request import Request, urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# opening the page we need and putting it in beautifulsoup \n",
    "req1 = Request('https://www.interest.co.nz/currencies/buying-foreign-currency', headers={'User-Agent': 'Mozilla/5.0'})\n",
    "tt1 = urlopen(req1)\n",
    "\n",
    "with open(\"ANZ.html\", \"wb\") as a: \n",
    "    while True:\n",
    "        chunk = tt1.read(1024) \n",
    "        if not chunk:\n",
    "            break\n",
    "        a.write(chunk)\n",
    "        \n",
    "with open(\"ANZ.html\", \"rb\") as a:\n",
    "    soup1 = BeautifulSoup(a.read(), \"html5lib\")\n",
    "\n",
    "# extract from the source page which we put in the variable 'soup1' all <tr> tags    \n",
    "rates=soup1.find_all('tr')\n",
    "rates=rates[2:15]\n",
    "\n",
    "# extract the price from the price tag using a loop that split text to get ANZ rate\n",
    "# save all ANZ rates in new_rates\n",
    "new_rates=[]\n",
    "for i in rates:\n",
    "    try:\n",
    "        # extract the third element for ANZ rate\n",
    "        new_rates=new_rates+[float(i.text.split()[3])]\n",
    "    except:\n",
    "        # if error occurs because ANZ does not exist in third element, extract the fourth element.\n",
    "        new_rates=new_rates+[float(i.text.split()[4])]\n",
    "        new_rates[3]=float(rates[3].text.split()[2])\n",
    "        \n",
    "# print a list with just the numerical rates of ANZ for the various currencies\n",
    "print(new_rates)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f975ad9",
   "metadata": {},
   "source": [
    "2.\tUse python to extract the prices of the scooters page from the Warehouse’s website\n",
    "https://www.thewarehouse.co.nz/c/sports-outdoors/scooters-skates/scooters\n",
    "The result should be a list with just the prices as numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "18ff7545-cff3-47d2-9296-50b0c6a96a28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[169.0, 349.0, 59.0, 69.0, 139.0, 199.0, 129.0, 649.0, 899.0, 59.0, 99.0, 69.0, 22.0, 22.0, 189.0, 129.0, 139.0, 199.0, 50.0, 35.0, 199.0, 199.0, 199.0, 299.0, 199.0, 189.0, 199.0, 99.0, 99.0, 199.0, 35.0, 29.0, 32.0, 35.0, 99.0, 99.0, 99.0, 29.0, 35.0, 35.0, 35.0]\n"
     ]
    }
   ],
   "source": [
    "# opening the page we need and putting it in beautifulsoup\n",
    "req2 = Request('https://www.thewarehouse.co.nz/c/sports-outdoors/scooters-skates/scooters?sz=64', headers={'User-Agent': 'Mozilla/5.0'})\n",
    "\n",
    "tt2 = urlopen(req2)\n",
    "with open(\"Scooter.html\", \"wb\") as b: \n",
    "    while True:\n",
    "        chunk = tt2.read(1024) \n",
    "        if not chunk:\n",
    "            break\n",
    "        b.write(chunk)\n",
    "        \n",
    "with open(\"Scooter.html\", \"rb\") as b:\n",
    "    soup2 = BeautifulSoup(b.read(), \"html5lib\")\n",
    "\n",
    "#extract from the source page which we put in the variable 'soup2' all div tags which have classes 'price-wrapper-left'    \n",
    "prices=soup2.find_all('div', class_=\"price-wrapper-left\")\n",
    "\n",
    "# extract the price from the price tag using a loop that split text and remove '$'\n",
    "# save all prices in price\n",
    "prices2=[]\n",
    "for i in prices:\n",
    "        prices2=prices2+[float(i.text.split()[0].replace(\"$\",\"\"))]\n",
    "        \n",
    "# print the prices of the scooters page from the Warehouse’s website\n",
    "print(prices2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acd9bc7a",
   "metadata": {},
   "source": [
    "3.\tUse python to extract ALL the prices of the scooters and skates page from the Warehouse’s website\n",
    "https://www.thewarehouse.co.nz/c/sports-outdoors/scooters-skates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "00c8ebac-8859-45a7-908e-70d833d173b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[169.0, 349.0, 30.0, 11.0, 59.0, 69.0, 29.98, 17.0, 139.0, 649.0, 899.0, 129.0, 30.0, 30.0, 15.0, 59.0, 59.0, 79.0, 39.0, 59.0, 39.0, 35.0, 28.0, 199.0, 45.0, 69.0, 99.0, 22.0, 28.0, 22.0, 59.0, 22.0, 189.0, 139.0, 129.0, 59.0, 28.0, 39.0, 28.0, 28.0, 199.0, 50.0, 199.0, 17.0, 25.0, 35.0, 35.0, 28.0, 28.0, 18.0, 19.0, 199.0, 20.0, 39.0, 79.0, 59.0, 199.0, 189.0, 299.0, 199.0, 99.0, 49.0, 99.0, 28.0, 99.0, 199.0, 99.0, 59.0, 50.0, 18.0, 199.0, 28.0, 39.0, 50.0, 39.0, 55.0, 28.0, 17.0, 17.0, 35.0, 35.0, 17.0, 35.0, 17.0, 17.0, 17.0, 25.0, 17.0, 35.0, 35.0, 35.0, 29.0, 35.0, 32.0, 25.0, 22.0, 35.0, 12.0, 29.0, 99.0, 25.0, 12.0, 99.0, 99.0, 29.0, 29.0, 189.0, 99.0, 69.0, 35.0, 19.0, 35.0, 25.0, 35.0, 169.0, 169.0, 189.0, 22.0, 169.0, 12.0, 30.0]\n"
     ]
    }
   ],
   "source": [
    "# opening the page we need and putting it in beautifulsoup\n",
    "\n",
    "req3 = Request('https://www.thewarehouse.co.nz/c/sports-outdoors/scooters-skates?sz=121', headers={'User-Agent': 'Mozilla/5.0'})\n",
    "\n",
    "tt3 = urlopen(req3)\n",
    "with open(\"scooterskates.html\", \"wb\") as c: \n",
    "    while True:\n",
    "        chunk = tt3.read(1024) \n",
    "        if not chunk:\n",
    "            break\n",
    "        c.write(chunk)\n",
    "        \n",
    "with open(\"scooterskates.html\", \"rb\") as c:\n",
    "    soup3 = BeautifulSoup(c.read(), \"html5lib\")\n",
    "\n",
    "# extract from the source page which we put in the variable 'soup2' all div tags which have classes 'price-wrapper-left'     \n",
    "prices3=soup3.find_all('div', class_=\"price-wrapper-left\")\n",
    "#print(prices3)\n",
    "\n",
    "# extract the price from the price tag using a loop that split text and remove '$'\n",
    "# save all prices in prices3\n",
    "price3=[]\n",
    "for i in prices3:\n",
    "        price3=price3+[float(i.text.split()[0].replace(\"$\",\"\"))]\n",
    "\n",
    "# print ALL the prices of the scooters and skates page from the Warehouse’s website\n",
    "print(price3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
